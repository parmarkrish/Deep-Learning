# Assignment Solutions for Stanford CS224n: NLP with Deep Learning
 
This repository contains my completed assignment solutions from [CS224n](https://web.stanford.edu/class/cs224n/).

### Assignment 1: Exploring Word Vectors
[Exploration of word vectors](https://github.com/parmarkrish/cs224n/blob/main/a1/exploring_word_vectors.ipynb)

### Assignment 2: Word2Vec
*Derivation of gradients of loss functions and implementation of Word2Vec, a technique to obtain word embeddings.* \
[Written Solutions](https://github.com/parmarkrish/cs224n/blob/main/a2_written/a2_written.pdf) | [Implementation](https://github.com/parmarkrish/cs224n/blob/main/a2/word2vec.py)

### Assignment 3: Dependency Parsing
*Implementation and exploration of neural network based method for uncovering grammatical relationships between words in a sentence.* \
[Written Solutions](https://github.com/parmarkrish/cs224n/blob/main/a3_written/a3_written.pdf) | [Implementation](https://github.com/parmarkrish/cs224n/blob/main/a3/parser_model.py)

### Assignment 4: Neural Machine Translation with RNNs
*Implementation and analysis of Seq2seq RNN with bidirectional LSTM encoder and unidirectional LSTM decoder.* \
[Written Solutions](https://github.com/parmarkrish/cs224n/blob/main/a4_written/a4_written.pdf) | [Implementation](https://github.com/parmarkrish/cs224n/blob/main/a4/nmt_model.py)

### Assignment 5: Self-Attention, Transformers, and Pretraining
*Analysis of attention mechanism and extension of a research codebase (implementation of pretraining techniques such as span corruption and transformer variants like PerceiverAR).* \
[Written Solutions](https://github.com/parmarkrish/cs224n/blob/main/a5_written/a5_written.pdf) | [Implementation](https://github.com/parmarkrish/cs224n/tree/main/a5/student_2023/src)

